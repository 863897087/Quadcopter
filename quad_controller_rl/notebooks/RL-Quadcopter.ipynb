{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Train a Quadcopter How to Fly\n",
    "\n",
    "Design an agent that can fly a quadcopter, and then train it using a reinforcement learning algorithm of your choice! Try to apply the techniques you have learnt, but also feel free to come up with innovative ideas and test them.\n",
    "\n",
    "## Task 1: Takeoff\n",
    "\n",
    "### Implement takeoff agent\n",
    "\n",
    "Train your agent so that it learns to successfully lift off from the ground. In order to do that, modify the `update()` method in `src/quad_controller_rl/rl_agent.py`, and any other supporting methods that might be necessary, to implement your reinforcement learning algorithm.\n",
    "\n",
    "The default set point (target) is 10 units above the floor. And the reward function is essentially the negative absolute distance from that set point (upto some threshold). See controller code (`src/quad_controller_rl/rl_controller.py`):\n",
    "\n",
    "```python\n",
    "reward = -min(abs(self.target - position), 20.0)\n",
    "```\n",
    "\n",
    "This is primarily meant for the Hover task (next), but should work for Takeoff as well.\n",
    "\n",
    "### Plot episode rewards\n",
    "\n",
    "Plot the episode rewards, either from a single run, or averaged over multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Read and plot episode rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Hover\n",
    "\n",
    "### Implement hover agent\n",
    "\n",
    "Now, your agent must take off and hover at the specified set point (default: 10 units above the floor). Same as before, modify the `update()` method (and any other supporting methods) to implement your reinforcement learning algorithm.\n",
    "\n",
    "### States and rewards\n",
    "\n",
    "In order for the agent to learn more efficiently, you may need to change the state representation you pass in (e.g. include acceleration, not just position and gravity), how the rewards are computed, etc. You can do this in the controller (`src/quad_controller_rl/rl_controller.py`).\n",
    "\n",
    "**Q**: Did you change the state representation or reward function in the controller? If so, please explain below what worked best for you, and why you chose that scheme. Include short code snippet(s) if needed.\n",
    "\n",
    "**A**: \n",
    "\n",
    "### Implementation notes\n",
    "\n",
    "**Q**: Discuss your implementation below briefly, using the following questions as a guide:\n",
    "\n",
    "- What algorithm(s) did you try? What worked best for you?\n",
    "- What was your final choice of hyperparameters (such as $\\alpha$, $\\gamma$, $\\epsilon$, etc.)?\n",
    "- What neural network architecture did you use (if any)? Specify layers, sizes, activation functions, etc.\n",
    "\n",
    "**A**:\n",
    "\n",
    "### Plot episode rewards\n",
    "\n",
    "As before, plot the episode rewards, either from a single run, or averaged over multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Read and plot episode rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Landing\n",
    "\n",
    "What goes up, must come down! But safely!\n",
    "\n",
    "### Implement landing agent\n",
    "\n",
    "This time, you will need to edit the starting state of the quadcopter to place it at a position above the floor (at least 10 units). And change the reward function to make the agent learn to settle down gently. For this purpose, you may need to edit the controller node (`scripts/rl_controller_node`, which is also a Python file).\n",
    "\n",
    "### Initial condition, states and rewards\n",
    "\n",
    "**Q**: Did you change the initial condition (starting state), state representation and/or reward function? If so, please explain below what worked best for you, and why you chose that scheme.\n",
    "\n",
    "**A**: \n",
    "\n",
    "### Implementation notes\n",
    "\n",
    "**Q**: Discuss your implementation below briefly, using the same questions as before to guide you.\n",
    "\n",
    "**A**:\n",
    "\n",
    "### Plot episode rewards\n",
    "\n",
    "As before, plot the episode rewards, either from a single run, or averaged over multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Read and plot episode rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Combined\n",
    "\n",
    "In order to design a complete flying system, you will need to incorporate all these basic behaviors into a single agent.\n",
    "\n",
    "### Setup end-to-end task\n",
    "\n",
    "The end-to-end task we are considering here is simply to takeoff, hover in-place for some duration, and then land. You will need to update the controller node (`scripts/rl_controller_node`) and controller (`src/quad_controller_rl/rl_controller.py`) to setup this task.\n",
    "\n",
    "**Q**: What changes did you make to setup the task? Explain briefly.\n",
    "\n",
    "**A**:\n",
    "\n",
    "### Implement combined agent\n",
    "\n",
    "Using your end-to-end task, implement the combined agent so that it learns to takeoff, hover and gently come back to ground level.\n",
    "\n",
    "### Combination scheme and implementation notes\n",
    "\n",
    "Now, it's up to you whether you want to train three separate (sub-)agents, or a single agent for the complete end-to-end task.\n",
    "\n",
    "**Q**: What did you end up doing? What challenges did you face, and how did you resolve them? Discuss any other implementation notes below.\n",
    "\n",
    "**A**:\n",
    "\n",
    "### Plot episode rewards\n",
    "\n",
    "As before, plot the episode rewards, either from a single run, or averaged over multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Read and plot episode rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflections\n",
    "\n",
    "**Q**: Briefly summarize your experience working on this project. You can use the following prompts for ideas.\n",
    "\n",
    "- What was the hardest part of the project? (e.g. getting started, running ROS, plotting, specific task, etc.)\n",
    "- How did you approach each task and choose an appropriate algorithm/implementation for it?\n",
    "- Did you find anything interesting in how the quadcopter or your agent behaved?\n",
    "\n",
    "**A**:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
